{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data aquisition, description and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I downloaded all the Italin parties manifesto from [WZB](https://manifesto-project.wzb.eu/). The first year the data are aviable is 1963 and the data are collected until 2001. After this year the format of the data change and it is not possible to integreta it in the code. Unfortunately I wasn't able to collect the party name, the column that specify the party is represented with a code.\n",
    "\n",
    "The data were downloaded oin a .csv format and cointains three columns, one containing the manifesto text and the other two are irrelevant (cmp_code and eu_code, mostly empty). The other information regarding the year and the party were stored in the name of each .csv. the function 'combine_csv' read all the document in a specific folder, combine them and extract the iseful informantion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data_assignment_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     final_df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n\u001b[1;32m---> 24\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_assignment_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m df\n",
      "Cell \u001b[1;32mIn [2], line 12\u001b[0m, in \u001b[0;36mcombine_csv\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis function combines all the csv files in the given path and returns a dataframe\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mdf: dataframe containing all the csv files\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     11\u001b[0m list_df \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m filename)\n\u001b[0;32m     14\u001b[0m     df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcmp_code\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meu_code\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# drop the columns (mostly nans)\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data_assignment_2'"
     ]
    }
   ],
   "source": [
    "def combine_csv(path):\n",
    "    '''\n",
    "    This function combines all the csv files in the given path and returns a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    path: path of the folder containing the csv files\n",
    "    \n",
    "    Returns:\n",
    "    df: dataframe containing all the csv files\n",
    "    '''\n",
    "    list_df = []\n",
    "    for filename in os.listdir(path):\n",
    "        df = pd.read_csv(path + '/'+ filename)\n",
    "        df.drop(columns = ['cmp_code', 'eu_code'], inplace = True) # drop the columns (mostly nans)\n",
    "        filename = filename.replace('.csv', '')\n",
    "        df['party_code'] = filename.split('_')[0] #extract the party code from the filename\n",
    "        df['year'] = filename.split('_')[1][:4] #extract the year from the filename\n",
    "        list_df.append(df)\n",
    "    final_df = pd.concat(list_df)\n",
    "    final_df.sort_values(['year'], inplace = True)\n",
    "    final_df.reset_index(inplace=True, drop=True)\n",
    "    return final_df\n",
    "\n",
    "df = combine_csv('data_assignment_2')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_new_party(df):\n",
    "    '''\n",
    "    This function checks if there is a new party in the dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    df: dataframe containing the data\n",
    "    \n",
    "    Returns:\n",
    "    new_party: list of tuples (new parties, year of their first appearance)\n",
    "    '''\n",
    "    first_year_df = df[df['year'] == df['year'].min()] #list of parties already present in the first year\n",
    "    list_party = list(first_year_df['party_code'])\n",
    "    new_party_year = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['party_code'] not in list_party: #check if the party is already present in the list\n",
    "            list_party.append(row['party_code']) #if not, add it to the party list\n",
    "            new_party_year.append((row['party_code'], row['year'])) #add the party and the year to the output list\n",
    "    return new_party_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('32220', '1972'),\n",
       " ('32320', '1972'),\n",
       " ('32310', '1976'),\n",
       " ('32520', '1976'),\n",
       " ('32211', '1983'),\n",
       " ('32110', '1987'),\n",
       " ('32212', '1992'),\n",
       " ('32720', '1994'),\n",
       " ('32610', '1994'),\n",
       " ('32528', '1994'),\n",
       " ('32951', '1994'),\n",
       " ('32521', '1996'),\n",
       " ('32321', '1996'),\n",
       " ('32529', '1996'),\n",
       " ('32213', '2001'),\n",
       " ('32111', '2001'),\n",
       " ('32522', '2001'),\n",
       " ('32421', '2001'),\n",
       " ('32611', '2001')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_new_party(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crating feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<79x30931 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 201474 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_manifestos = list(df.text) \n",
    "list_manifestos = list(map(lambda x: x.lower(), list_manifestos)) # convert all in lower characthers\n",
    "vectorizer = TfidfVectorizer(stop_words=get_stop_words('italian'))\n",
    "dfm_manifestos = vectorizer.fit_transform(list_manifestos)\n",
    "dfm_manifestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use this method to created the feature matrix. TFIDF is the best tecnique to address this kind of quesiton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a political science prospective new parties arise because of lack of interest in some hot topic that matters for the citizens. It will be interesting to investigate if the Manifesto of those new party match the theory behaind. The idea is check the most mention topic in the manifestos and compare them with the already present ones. \n",
    "\n",
    "The question that will be answered is: does the new party tackle issue that were not faced before? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    n_components = model.n_components\n",
    "    fig, axes = plt.subplots(n_components//5,5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    t_titles = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "        \n",
    "        t_titles.append(\", \".join(top_features[:3]))\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return t_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_feature_names', '_check_n_features', '_check_params', '_check_w_h', '_fit_transform', '_get_param_names', '_get_tags', '_more_tags', '_n_features_out', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_scale_regularization', '_validate_data', 'alpha', 'alpha_H', 'alpha_W', 'beta_loss', 'fit', 'fit_transform', 'get_feature_names_out', 'get_params', 'init', 'inverse_transform', 'l1_ratio', 'max_iter', 'n_components', 'random_state', 'regularization', 'set_params', 'shuffle', 'solver', 'tol', 'transform', 'verbose']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NMF' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nmf \u001b[39m=\u001b[39m NMF(\u001b[39m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mdir\u001b[39m(nmf))\n\u001b[1;32m----> 3\u001b[0m nmf\u001b[39m.\u001b[39;49m_n_features_out\n",
      "File \u001b[1;32mc:\\Users\\zazzo\\.virtualenvs\\Text_as_data-P4hExxh8\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770\u001b[0m, in \u001b[0;36mNMF._n_features_out\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1767\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   1768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_n_features_out\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1769\u001b[0m     \u001b[39m\"\"\"Number of transformed output features.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1770\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcomponents_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NMF' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "nmf = NMF(10)\n",
    "print(dir(nmf))\n",
    "nmf._n_features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NMF' object has no attribute 'n_components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m vec \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39mget_stop_words(\u001b[39m'\u001b[39m\u001b[39mitalian\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m vec\u001b[39m.\u001b[39mfit_transform(df_2001\u001b[39m.\u001b[39mtext)\n\u001b[1;32m----> 8\u001b[0m t_titles \u001b[39m=\u001b[39m plot_top_words(nmf, vec\u001b[39m.\u001b[39;49mget_feature_names_out(df_2001\u001b[39m.\u001b[39;49mtext), \u001b[39m8\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTopics researched by the Hertie School\u001b[39;49m\u001b[39m\"\u001b[39;49m, )\n",
      "Cell \u001b[1;32mIn [31], line 2\u001b[0m, in \u001b[0;36mplot_top_words\u001b[1;34m(model, feature_names, n_top_words, title)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_top_words\u001b[39m(model, feature_names, n_top_words, title):\n\u001b[1;32m----> 2\u001b[0m     n_components \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mn_components_\n\u001b[0;32m      3\u001b[0m     fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(n_components\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m, \u001b[39m15\u001b[39m), sharex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     axes \u001b[39m=\u001b[39m axes\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NMF' object has no attribute 'n_components_'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_2001 = df[df['year'] == '2001']\n",
    "nmf = NMF(10)\n",
    "vec = TfidfVectorizer(stop_words=get_stop_words('italian'))\n",
    "vec.fit_transform(df_2001.text)\n",
    "t_titles = plot_top_words(nmf, vec.get_feature_names_out(df_2001.text), 8, \"Topics researched by the Hertie School\", )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "97b7f0f1662268f201b31cfd395afa89f3d7f38c324c5052cf221b976bda3ef3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
